{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSmtbUCUv39zUGT2d5fJT0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghulchandramouli/breaking-into-Computational-Graphs-/blob/main/Automatic_Differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoGrad\n",
        "> Automatic differentiation (AD) is a set of techniques used to evaluate the partial derivative of a function specified by a computer program. It allows for efficient and accurate calculation of derivatives of numeric functions expressed as computer programs.\n",
        "\n"
      ],
      "metadata": {
        "id": "JS8-_NSN0QO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If requires_grad = True, the tensor object keeps track of how it was created\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
        "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
        "\n",
        "# Notice that both of x and y have their required_grad to true, therefore we an comoute gradients with respect to them\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "# z knows that it was created as a result of addition of x and y, It knows that it wasn't read in from a file\n",
        "print(z.grad_fn)\n",
        "# And if we fo further on this\n",
        "s = z.sum()\n",
        "print(s)\n",
        "print(s.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz39Zdun0S5M",
        "outputId": "a7391c07-cec4-43c3-fc42-26542235e95a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<AddBackward0 object at 0x79f442688d60>\n",
            "tensor(21., grad_fn=<SumBackward0>)\n",
            "<SumBackward0 object at 0x79f442688f40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now if we backpropagate on s, we can find the gradients of s with respect to x:\n",
        "s.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_AGv2Yw0lP8",
        "outputId": "fcd9e29e-4363-4834-c3ad-790ebb4907df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# By default, Tensors have `requires_grad = False`\n",
        "x = torch.randn(2, 2)\n",
        "y = torch.randn(2, 2)\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x + y\n",
        "\n",
        "# So you can't backprop through z\n",
        "print(z.grad_fn)\n",
        "\n",
        "# Another way to set the required_grad is true:\n",
        "x.requires_grad_()\n",
        "y.requires_grad_()\n",
        "\n",
        "# z contains enough information to compute gradients, as we saw above\n",
        "z = x + y\n",
        "print(z.grad_fn)\n",
        "\n",
        "# If any input to an operation has ``requires_grad=True``, so will the output\n",
        "print(z.requires_grad)\n",
        "\n",
        "# Now z has the computation history that relates itself to x and y\n",
        "# Can we just take its values, and **detach** it from its history?\n",
        "\n",
        "new_z = z.detach()\n",
        "print(new_z.grad_fn)\n",
        "# ... does new_z have information to backprop to x and y?\n",
        "# NO!\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x + 10).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x + 10).requires_grad)"
      ],
      "metadata": {
        "id": "QhVDqvOP1bVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# last example:\n",
        "x = torch.ones(2,2, requires_grad=True)\n",
        "print(x)\n",
        "y = x + 2\n",
        "print(y)\n",
        "print(y.grad_fn)\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "print(z, out)\n",
        "out.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QQsUDcN1nIO",
        "outputId": "9f61cacb-0e04-4dae-f5eb-b88dd12f92b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x79f44282c850>\n",
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99f2O15O3ewk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}